WARNING: Logging before InitGoogleLogging() is written to STDERR
I0928 21:59:57.292006 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "data"
input: "label"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 5
  dim: 565
}
input_shape {
  dim: 70
}
layer {
  name: "one"
  type: "Python"
  bottom: "data"
  bottom: "label"
  top: "a2a"
  top: "a2s"
  top: "s2a"
  python_param {
    module: "test_simplified_message_in"
    layer: "test_message_in"
  }
}
I0928 21:59:57.292084 31416 net.cpp:344] Input 0 -> data
I0928 21:59:57.292134 31416 net.cpp:344] Input 1 -> label
I0928 21:59:57.292146 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.292171 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.292225 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.292234 31416 net.cpp:384] one <- data
I0928 21:59:57.292244 31416 net.cpp:384] one <- label
I0928 21:59:57.292254 31416 net.cpp:342] one -> a2a
I0928 21:59:57.292268 31416 net.cpp:342] one -> a2s
I0928 21:59:57.292279 31416 net.cpp:342] one -> s2a
I0928 21:59:57.292289 31416 net.cpp:113] Setting up one
I0928 21:59:57.292503 31416 net.cpp:120] Top shape: 202 40 (8080)
I0928 21:59:57.292512 31416 net.cpp:120] Top shape: 24 40 (960)
I0928 21:59:57.292516 31416 net.cpp:120] Top shape: 24 5 (120)
I0928 21:59:57.292518 31416 net.cpp:126] Memory required for data: 36640
I0928 21:59:57.292526 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.292531 31416 net.cpp:208] This network produces output a2a
I0928 21:59:57.292536 31416 net.cpp:208] This network produces output a2s
I0928 21:59:57.292541 31416 net.cpp:208] This network produces output s2a
I0928 21:59:57.292548 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.292552 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.292554 31416 net.cpp:222] Memory required for data: 36640
.I0928 21:59:57.423421 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "data"
input: "label"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 5
  dim: 565
}
input_shape {
  dim: 70
}
layer {
  name: "one"
  type: "Python"
  bottom: "data"
  bottom: "label"
  top: "a2a"
  top: "a2s"
  top: "s2a"
  python_param {
    module: "test_simplified_message_in"
    layer: "test_message_in"
  }
}
I0928 21:59:57.423485 31416 net.cpp:344] Input 0 -> data
I0928 21:59:57.423518 31416 net.cpp:344] Input 1 -> label
I0928 21:59:57.423545 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.423573 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.423614 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.423622 31416 net.cpp:384] one <- data
I0928 21:59:57.423634 31416 net.cpp:384] one <- label
I0928 21:59:57.423643 31416 net.cpp:342] one -> a2a
I0928 21:59:57.423657 31416 net.cpp:342] one -> a2s
I0928 21:59:57.423667 31416 net.cpp:342] one -> s2a
I0928 21:59:57.423677 31416 net.cpp:113] Setting up one
I0928 21:59:57.423838 31416 net.cpp:120] Top shape: 202 40 (8080)
I0928 21:59:57.423847 31416 net.cpp:120] Top shape: 24 40 (960)
I0928 21:59:57.423851 31416 net.cpp:120] Top shape: 24 5 (120)
I0928 21:59:57.423853 31416 net.cpp:126] Memory required for data: 36640
I0928 21:59:57.423861 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.423866 31416 net.cpp:208] This network produces output a2a
I0928 21:59:57.423871 31416 net.cpp:208] This network produces output a2s
I0928 21:59:57.423876 31416 net.cpp:208] This network produces output s2a
I0928 21:59:57.423882 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.423887 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.423888 31416 net.cpp:222] Memory required for data: 36640
.I0928 21:59:57.440750 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "data"
input: "label"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 5
  dim: 565
}
input_shape {
  dim: 70
}
layer {
  name: "one"
  type: "Python"
  bottom: "data"
  bottom: "label"
  top: "a2a"
  top: "a2s"
  top: "s2a"
  python_param {
    module: "test_simplified_message_in"
    layer: "test_message_in"
  }
}
I0928 21:59:57.440786 31416 net.cpp:344] Input 0 -> data
I0928 21:59:57.440806 31416 net.cpp:344] Input 1 -> label
I0928 21:59:57.440819 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.440831 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.440861 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.440868 31416 net.cpp:384] one <- data
I0928 21:59:57.440878 31416 net.cpp:384] one <- label
I0928 21:59:57.440888 31416 net.cpp:342] one -> a2a
I0928 21:59:57.440901 31416 net.cpp:342] one -> a2s
I0928 21:59:57.440912 31416 net.cpp:342] one -> s2a
I0928 21:59:57.440922 31416 net.cpp:113] Setting up one
I0928 21:59:57.441063 31416 net.cpp:120] Top shape: 202 40 (8080)
I0928 21:59:57.441072 31416 net.cpp:120] Top shape: 24 40 (960)
I0928 21:59:57.441076 31416 net.cpp:120] Top shape: 24 5 (120)
I0928 21:59:57.441078 31416 net.cpp:126] Memory required for data: 36640
I0928 21:59:57.441086 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.441088 31416 net.cpp:208] This network produces output a2a
I0928 21:59:57.441095 31416 net.cpp:208] This network produces output a2s
I0928 21:59:57.441099 31416 net.cpp:208] This network produces output s2a
I0928 21:59:57.441107 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.441109 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.441112 31416 net.cpp:222] Memory required for data: 36640
.I0928 21:59:57.441400 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.441453 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.441483 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.441510 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.441517 31416 net.cpp:342] data -> data
I0928 21:59:57.441535 31416 net.cpp:342] data -> label
I0928 21:59:57.441547 31416 net.cpp:113] Setting up data
I0928 21:59:57.441596 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.441603 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.441606 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.441612 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.441637 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.441643 31416 net.cpp:384] conv <- data
I0928 21:59:57.441665 31416 net.cpp:342] conv -> conv
I0928 21:59:57.441689 31416 net.cpp:113] Setting up conv
I0928 21:59:57.442092 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.442098 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.442116 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.442129 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.442134 31416 net.cpp:384] ip <- conv
I0928 21:59:57.442147 31416 net.cpp:342] ip -> ip
I0928 21:59:57.442158 31416 net.cpp:113] Setting up ip
I0928 21:59:57.442873 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.442878 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.442893 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.442905 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.442910 31416 net.cpp:384] loss <- ip
I0928 21:59:57.442919 31416 net.cpp:384] loss <- label
I0928 21:59:57.442926 31416 net.cpp:342] loss -> loss
I0928 21:59:57.442939 31416 net.cpp:113] Setting up loss
I0928 21:59:57.442945 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.442970 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.442973 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.442981 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.442986 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.442991 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.442993 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.442996 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.443001 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.443011 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.443017 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.443018 31416 net.cpp:222] Memory required for data: 16604
.I0928 21:59:57.443790 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.443835 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.443855 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.443868 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.443874 31416 net.cpp:342] data -> data
I0928 21:59:57.443892 31416 net.cpp:342] data -> label
I0928 21:59:57.443904 31416 net.cpp:113] Setting up data
I0928 21:59:57.443925 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.443931 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.443933 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.443938 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.443953 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.443958 31416 net.cpp:384] conv <- data
I0928 21:59:57.443970 31416 net.cpp:342] conv -> conv
I0928 21:59:57.443984 31416 net.cpp:113] Setting up conv
I0928 21:59:57.444016 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.444021 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.444036 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.444048 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.444053 31416 net.cpp:384] ip <- conv
I0928 21:59:57.444064 31416 net.cpp:342] ip -> ip
I0928 21:59:57.444075 31416 net.cpp:113] Setting up ip
I0928 21:59:57.444792 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.444797 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.444809 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.444818 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.444823 31416 net.cpp:384] loss <- ip
I0928 21:59:57.444833 31416 net.cpp:384] loss <- label
I0928 21:59:57.444840 31416 net.cpp:342] loss -> loss
I0928 21:59:57.444851 31416 net.cpp:113] Setting up loss
I0928 21:59:57.444860 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.444882 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.444886 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.444891 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.444895 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.444900 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.444902 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.444905 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.444911 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.444918 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.444924 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.444927 31416 net.cpp:222] Memory required for data: 16604
.I0928 21:59:57.445369 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.445413 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.445432 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.445446 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.445452 31416 net.cpp:342] data -> data
I0928 21:59:57.445472 31416 net.cpp:342] data -> label
I0928 21:59:57.445485 31416 net.cpp:113] Setting up data
I0928 21:59:57.445505 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.445511 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.445514 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.445519 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.445533 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.445538 31416 net.cpp:384] conv <- data
I0928 21:59:57.445554 31416 net.cpp:342] conv -> conv
I0928 21:59:57.445566 31416 net.cpp:113] Setting up conv
I0928 21:59:57.445600 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.445605 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.445619 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.445633 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.445638 31416 net.cpp:384] ip <- conv
I0928 21:59:57.445652 31416 net.cpp:342] ip -> ip
I0928 21:59:57.445662 31416 net.cpp:113] Setting up ip
I0928 21:59:57.446395 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.446401 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.446414 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.446426 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.446431 31416 net.cpp:384] loss <- ip
I0928 21:59:57.446440 31416 net.cpp:384] loss <- label
I0928 21:59:57.446449 31416 net.cpp:342] loss -> loss
I0928 21:59:57.446461 31416 net.cpp:113] Setting up loss
I0928 21:59:57.446467 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.446488 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.446493 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.446497 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.446501 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.446509 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.446512 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.446516 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.446519 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.446529 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.446537 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.446538 31416 net.cpp:222] Memory required for data: 16604
.I0928 21:59:57.447247 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.447289 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.447309 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.447322 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.447329 31416 net.cpp:342] data -> data
I0928 21:59:57.447350 31416 net.cpp:342] data -> label
I0928 21:59:57.447386 31416 net.cpp:113] Setting up data
I0928 21:59:57.447417 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.447427 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.447429 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.447444 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.447459 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.447465 31416 net.cpp:384] conv <- data
I0928 21:59:57.447479 31416 net.cpp:342] conv -> conv
I0928 21:59:57.447492 31416 net.cpp:113] Setting up conv
I0928 21:59:57.447525 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.447530 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.447543 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.447556 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.447561 31416 net.cpp:384] ip <- conv
I0928 21:59:57.447573 31416 net.cpp:342] ip -> ip
I0928 21:59:57.447583 31416 net.cpp:113] Setting up ip
I0928 21:59:57.448285 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.448290 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.448302 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.448312 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.448317 31416 net.cpp:384] loss <- ip
I0928 21:59:57.448325 31416 net.cpp:384] loss <- label
I0928 21:59:57.448333 31416 net.cpp:342] loss -> loss
I0928 21:59:57.448343 31416 net.cpp:113] Setting up loss
I0928 21:59:57.448348 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.448369 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.448374 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.448379 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.448382 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.448386 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.448390 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.448392 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.448397 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.448410 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.448416 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.448418 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.448614 31416 net.cpp:731] Serializing 4 layers
I0928 21:59:57.449182 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.449226 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.449245 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.449259 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.449265 31416 net.cpp:342] data -> data
I0928 21:59:57.449282 31416 net.cpp:342] data -> label
I0928 21:59:57.449295 31416 net.cpp:113] Setting up data
I0928 21:59:57.449316 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.449321 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.449324 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.449329 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.449342 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.449348 31416 net.cpp:384] conv <- data
I0928 21:59:57.449362 31416 net.cpp:342] conv -> conv
I0928 21:59:57.449374 31416 net.cpp:113] Setting up conv
I0928 21:59:57.449406 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.449411 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.449425 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.449439 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.449443 31416 net.cpp:384] ip <- conv
I0928 21:59:57.449455 31416 net.cpp:342] ip -> ip
I0928 21:59:57.449465 31416 net.cpp:113] Setting up ip
I0928 21:59:57.450193 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.450199 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.450212 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.450222 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.450227 31416 net.cpp:384] loss <- ip
I0928 21:59:57.450235 31416 net.cpp:384] loss <- label
I0928 21:59:57.450242 31416 net.cpp:342] loss -> loss
I0928 21:59:57.450253 31416 net.cpp:113] Setting up loss
I0928 21:59:57.450259 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.450280 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.450284 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.450289 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.450292 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.450297 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.450300 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.450304 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.450307 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.450316 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.450322 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.450325 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.450573 31416 net.cpp:704] Copying source layer data
I0928 21:59:57.450579 31416 net.cpp:704] Copying source layer conv
I0928 21:59:57.450587 31416 net.cpp:704] Copying source layer ip
I0928 21:59:57.450675 31416 net.cpp:704] Copying source layer loss
.I0928 21:59:57.452142 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "a2a"
input: "a2s"
input: "s2a"
input: "label"
input: "unaries"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 202
  dim: 40
}
input_shape {
  dim: 24
  dim: 5
}
input_shape {
  dim: 24
  dim: 40
}
input_shape {
  dim: 70
}
input_shape {
  dim: 5
  dim: 565
}
layer {
  name: "one"
  type: "Python"
  bottom: "a2a"
  bottom: "a2s"
  bottom: "s2a"
  bottom: "label"
  bottom: "unaries"
  top: "scene"
  top: "action"
  python_param {
    module: "test_simplified_message_out"
    layer: "test_message_out"
  }
}
I0928 21:59:57.452183 31416 net.cpp:344] Input 0 -> a2a
I0928 21:59:57.452204 31416 net.cpp:344] Input 1 -> a2s
I0928 21:59:57.452219 31416 net.cpp:344] Input 2 -> s2a
I0928 21:59:57.452229 31416 net.cpp:344] Input 3 -> label
I0928 21:59:57.452237 31416 net.cpp:344] Input 4 -> unaries
I0928 21:59:57.452255 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.452268 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.452299 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.452307 31416 net.cpp:384] one <- a2a
I0928 21:59:57.452317 31416 net.cpp:384] one <- a2s
I0928 21:59:57.452323 31416 net.cpp:384] one <- s2a
I0928 21:59:57.452328 31416 net.cpp:384] one <- label
I0928 21:59:57.452332 31416 net.cpp:384] one <- unaries
I0928 21:59:57.452340 31416 net.cpp:342] one -> scene
I0928 21:59:57.452353 31416 net.cpp:342] one -> action
I0928 21:59:57.452360 31416 net.cpp:113] Setting up one
I0928 21:59:57.452503 31416 net.cpp:120] Top shape: 5 10 (50)
I0928 21:59:57.452512 31416 net.cpp:120] Top shape: 70 120 (8400)
I0928 21:59:57.452514 31416 net.cpp:126] Memory required for data: 33800
I0928 21:59:57.452522 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.452525 31416 net.cpp:208] This network produces output action
I0928 21:59:57.452533 31416 net.cpp:208] This network produces output scene
I0928 21:59:57.452540 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.452543 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.452546 31416 net.cpp:222] Memory required for data: 33800
.I0928 21:59:57.475360 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "a2a"
input: "a2s"
input: "s2a"
input: "label"
input: "unaries"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 202
  dim: 40
}
input_shape {
  dim: 24
  dim: 5
}
input_shape {
  dim: 24
  dim: 40
}
input_shape {
  dim: 70
}
input_shape {
  dim: 5
  dim: 565
}
layer {
  name: "one"
  type: "Python"
  bottom: "a2a"
  bottom: "a2s"
  bottom: "s2a"
  bottom: "label"
  bottom: "unaries"
  top: "scene"
  top: "action"
  python_param {
    module: "test_simplified_message_out"
    layer: "test_message_out"
  }
}
I0928 21:59:57.475427 31416 net.cpp:344] Input 0 -> a2a
I0928 21:59:57.475463 31416 net.cpp:344] Input 1 -> a2s
I0928 21:59:57.475484 31416 net.cpp:344] Input 2 -> s2a
I0928 21:59:57.475500 31416 net.cpp:344] Input 3 -> label
I0928 21:59:57.475512 31416 net.cpp:344] Input 4 -> unaries
I0928 21:59:57.475527 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.475549 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.475602 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.475615 31416 net.cpp:384] one <- a2a
I0928 21:59:57.475632 31416 net.cpp:384] one <- a2s
I0928 21:59:57.475642 31416 net.cpp:384] one <- s2a
I0928 21:59:57.475651 31416 net.cpp:384] one <- label
I0928 21:59:57.475657 31416 net.cpp:384] one <- unaries
I0928 21:59:57.475672 31416 net.cpp:342] one -> scene
I0928 21:59:57.475692 31416 net.cpp:342] one -> action
I0928 21:59:57.475704 31416 net.cpp:113] Setting up one
I0928 21:59:57.475963 31416 net.cpp:120] Top shape: 5 10 (50)
I0928 21:59:57.475977 31416 net.cpp:120] Top shape: 70 120 (8400)
I0928 21:59:57.475988 31416 net.cpp:126] Memory required for data: 33800
I0928 21:59:57.476001 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.476007 31416 net.cpp:208] This network produces output action
I0928 21:59:57.476017 31416 net.cpp:208] This network produces output scene
I0928 21:59:57.476032 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.476037 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.476042 31416 net.cpp:222] Memory required for data: 33800
.I0928 21:59:57.486707 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "a2a"
input: "a2s"
input: "s2a"
input: "label"
input: "unaries"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 202
  dim: 40
}
input_shape {
  dim: 24
  dim: 5
}
input_shape {
  dim: 24
  dim: 40
}
input_shape {
  dim: 70
}
input_shape {
  dim: 5
  dim: 565
}
layer {
  name: "one"
  type: "Python"
  bottom: "a2a"
  bottom: "a2s"
  bottom: "s2a"
  bottom: "label"
  bottom: "unaries"
  top: "scene"
  top: "action"
  python_param {
    module: "test_simplified_message_out"
    layer: "test_message_out"
  }
}
I0928 21:59:57.486747 31416 net.cpp:344] Input 0 -> a2a
I0928 21:59:57.486768 31416 net.cpp:344] Input 1 -> a2s
I0928 21:59:57.486780 31416 net.cpp:344] Input 2 -> s2a
I0928 21:59:57.486790 31416 net.cpp:344] Input 3 -> label
I0928 21:59:57.486799 31416 net.cpp:344] Input 4 -> unaries
I0928 21:59:57.486806 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.486820 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.486853 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.486861 31416 net.cpp:384] one <- a2a
I0928 21:59:57.486871 31416 net.cpp:384] one <- a2s
I0928 21:59:57.486877 31416 net.cpp:384] one <- s2a
I0928 21:59:57.486882 31416 net.cpp:384] one <- label
I0928 21:59:57.486886 31416 net.cpp:384] one <- unaries
I0928 21:59:57.486894 31416 net.cpp:342] one -> scene
I0928 21:59:57.486907 31416 net.cpp:342] one -> action
I0928 21:59:57.486914 31416 net.cpp:113] Setting up one
I0928 21:59:57.487071 31416 net.cpp:120] Top shape: 5 10 (50)
I0928 21:59:57.487079 31416 net.cpp:120] Top shape: 70 120 (8400)
I0928 21:59:57.487082 31416 net.cpp:126] Memory required for data: 33800
I0928 21:59:57.487089 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.487093 31416 net.cpp:208] This network produces output action
I0928 21:59:57.487099 31416 net.cpp:208] This network produces output scene
I0928 21:59:57.487107 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.487112 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.487113 31416 net.cpp:222] Memory required for data: 33800
.I0928 21:59:57.487318 31416 solver.cpp:32] Initializing solver from parameters: 
test_iter: 10
test_interval: 10
base_lr: 0.01
display: 100
max_iter: 100
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
net: "/tmp/tmpSnjaLu"
snapshot_after_train: false
I0928 21:59:57.487334 31416 solver.cpp:70] Creating training net from net file: /tmp/tmpSnjaLu
I0928 21:59:57.487465 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.487511 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.487530 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.487545 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.487551 31416 net.cpp:342] data -> data
I0928 21:59:57.487570 31416 net.cpp:342] data -> label
I0928 21:59:57.487582 31416 net.cpp:113] Setting up data
I0928 21:59:57.487604 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.487610 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.487613 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.487618 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.487633 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.487638 31416 net.cpp:384] conv <- data
I0928 21:59:57.487651 31416 net.cpp:342] conv -> conv
I0928 21:59:57.487664 31416 net.cpp:113] Setting up conv
I0928 21:59:57.487700 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.487704 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.487720 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.487732 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.487737 31416 net.cpp:384] ip <- conv
I0928 21:59:57.487750 31416 net.cpp:342] ip -> ip
I0928 21:59:57.487761 31416 net.cpp:113] Setting up ip
I0928 21:59:57.488584 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.488589 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.488600 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.488610 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.488615 31416 net.cpp:384] loss <- ip
I0928 21:59:57.488623 31416 net.cpp:384] loss <- label
I0928 21:59:57.488631 31416 net.cpp:342] loss -> loss
I0928 21:59:57.488641 31416 net.cpp:113] Setting up loss
I0928 21:59:57.488647 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.488668 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.488673 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.488678 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.488682 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.488687 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.488689 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.488692 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.488698 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.488706 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.488713 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.488715 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.488811 31416 solver.cpp:154] Creating test net (#0) specified by net file: /tmp/tmpSnjaLu
I0928 21:59:57.488863 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.488909 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.488927 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.488940 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.488947 31416 net.cpp:342] data -> data
I0928 21:59:57.488965 31416 net.cpp:342] data -> label
I0928 21:59:57.488976 31416 net.cpp:113] Setting up data
I0928 21:59:57.488996 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.489002 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.489006 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.489011 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.489023 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.489029 31416 net.cpp:384] conv <- data
I0928 21:59:57.489042 31416 net.cpp:342] conv -> conv
I0928 21:59:57.489055 31416 net.cpp:113] Setting up conv
I0928 21:59:57.489086 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.489091 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.489106 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.489120 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.489125 31416 net.cpp:384] ip <- conv
I0928 21:59:57.489136 31416 net.cpp:342] ip -> ip
I0928 21:59:57.489146 31416 net.cpp:113] Setting up ip
I0928 21:59:57.489850 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.489855 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.489866 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.489889 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.489917 31416 net.cpp:384] loss <- ip
I0928 21:59:57.489928 31416 net.cpp:384] loss <- label
I0928 21:59:57.489934 31416 net.cpp:342] loss -> loss
I0928 21:59:57.489961 31416 net.cpp:113] Setting up loss
I0928 21:59:57.489967 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.489989 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.489994 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.489998 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.490002 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.490006 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.490010 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.490012 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.490017 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.490025 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.490031 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.490034 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.490049 31416 solver.cpp:42] Solver scaffolding done.
I0928 21:59:57.490110 31416 solver.cpp:32] Initializing solver from parameters: 
test_iter: 10
test_interval: 10
base_lr: 0.01
display: 100
max_iter: 100
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
net: "/tmp/tmpSnjaLu"
snapshot_after_train: false
I0928 21:59:57.490120 31416 solver.cpp:70] Creating training net from net file: /tmp/tmpSnjaLu
I0928 21:59:57.490239 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.490285 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.490305 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.490319 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.490325 31416 net.cpp:342] data -> data
I0928 21:59:57.490344 31416 net.cpp:342] data -> label
I0928 21:59:57.490356 31416 net.cpp:113] Setting up data
I0928 21:59:57.490376 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.490382 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.490384 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.490389 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.490403 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.490409 31416 net.cpp:384] conv <- data
I0928 21:59:57.490422 31416 net.cpp:342] conv -> conv
I0928 21:59:57.490435 31416 net.cpp:113] Setting up conv
I0928 21:59:57.490468 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.490471 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.490486 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.490499 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.490504 31416 net.cpp:384] ip <- conv
I0928 21:59:57.490515 31416 net.cpp:342] ip -> ip
I0928 21:59:57.490526 31416 net.cpp:113] Setting up ip
I0928 21:59:57.491253 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.491258 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.491271 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.491281 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.491284 31416 net.cpp:384] loss <- ip
I0928 21:59:57.491293 31416 net.cpp:384] loss <- label
I0928 21:59:57.491300 31416 net.cpp:342] loss -> loss
I0928 21:59:57.491312 31416 net.cpp:113] Setting up loss
I0928 21:59:57.491317 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.491338 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.491343 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.491346 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.491350 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.491354 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.491358 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.491360 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.491365 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.491374 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.491380 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.491382 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.491475 31416 solver.cpp:154] Creating test net (#0) specified by net file: /tmp/tmpSnjaLu
I0928 21:59:57.491528 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.491578 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.491606 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.491623 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.491642 31416 net.cpp:342] data -> data
I0928 21:59:57.491680 31416 net.cpp:342] data -> label
I0928 21:59:57.491703 31416 net.cpp:113] Setting up data
I0928 21:59:57.491734 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.491739 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.491751 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.491756 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.491771 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.491777 31416 net.cpp:384] conv <- data
I0928 21:59:57.491791 31416 net.cpp:342] conv -> conv
I0928 21:59:57.491803 31416 net.cpp:113] Setting up conv
I0928 21:59:57.491837 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.491842 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.491857 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.491869 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.491874 31416 net.cpp:384] ip <- conv
I0928 21:59:57.491886 31416 net.cpp:342] ip -> ip
I0928 21:59:57.491896 31416 net.cpp:113] Setting up ip
I0928 21:59:57.492610 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.492616 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.492629 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.492640 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.492645 31416 net.cpp:384] loss <- ip
I0928 21:59:57.492652 31416 net.cpp:384] loss <- label
I0928 21:59:57.492660 31416 net.cpp:342] loss -> loss
I0928 21:59:57.492671 31416 net.cpp:113] Setting up loss
I0928 21:59:57.492676 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.492698 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.492703 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.492707 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.492712 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.492717 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.492719 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.492722 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.492727 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.492735 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.492741 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.492743 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.492758 31416 solver.cpp:42] Solver scaffolding done.
.I0928 21:59:57.493963 31416 solver.cpp:32] Initializing solver from parameters: 
test_iter: 10
test_interval: 10
base_lr: 0.01
display: 100
max_iter: 100
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
net: "/tmp/tmpsemI8J"
snapshot_after_train: false
I0928 21:59:57.493976 31416 solver.cpp:70] Creating training net from net file: /tmp/tmpsemI8J
I0928 21:59:57.494099 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.494145 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.494164 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.494179 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.494185 31416 net.cpp:342] data -> data
I0928 21:59:57.494204 31416 net.cpp:342] data -> label
I0928 21:59:57.494216 31416 net.cpp:113] Setting up data
I0928 21:59:57.494236 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.494242 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.494246 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.494251 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.494264 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.494269 31416 net.cpp:384] conv <- data
I0928 21:59:57.494282 31416 net.cpp:342] conv -> conv
I0928 21:59:57.494295 31416 net.cpp:113] Setting up conv
I0928 21:59:57.494330 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.494335 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.494350 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.494364 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.494369 31416 net.cpp:384] ip <- conv
I0928 21:59:57.494380 31416 net.cpp:342] ip -> ip
I0928 21:59:57.494390 31416 net.cpp:113] Setting up ip
I0928 21:59:57.495100 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.495105 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.495116 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.495126 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.495131 31416 net.cpp:384] loss <- ip
I0928 21:59:57.495139 31416 net.cpp:384] loss <- label
I0928 21:59:57.495147 31416 net.cpp:342] loss -> loss
I0928 21:59:57.495158 31416 net.cpp:113] Setting up loss
I0928 21:59:57.495163 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.495184 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.495188 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.495193 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.495198 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.495201 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.495204 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.495208 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.495213 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.495221 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.495228 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.495230 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.495321 31416 solver.cpp:154] Creating test net (#0) specified by net file: /tmp/tmpsemI8J
I0928 21:59:57.495373 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.495414 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.495432 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.495445 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.495456 31416 net.cpp:342] data -> data
I0928 21:59:57.495475 31416 net.cpp:342] data -> label
I0928 21:59:57.495486 31416 net.cpp:113] Setting up data
I0928 21:59:57.495507 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.495513 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.495515 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.495520 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.495534 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.495539 31416 net.cpp:384] conv <- data
I0928 21:59:57.495553 31416 net.cpp:342] conv -> conv
I0928 21:59:57.495565 31416 net.cpp:113] Setting up conv
I0928 21:59:57.495597 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.495602 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.495616 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.495630 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.495635 31416 net.cpp:384] ip <- conv
I0928 21:59:57.495646 31416 net.cpp:342] ip -> ip
I0928 21:59:57.495656 31416 net.cpp:113] Setting up ip
I0928 21:59:57.496359 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.496364 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.496376 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.496397 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.496400 31416 net.cpp:384] loss <- ip
I0928 21:59:57.496409 31416 net.cpp:384] loss <- label
I0928 21:59:57.496428 31416 net.cpp:342] loss -> loss
I0928 21:59:57.496438 31416 net.cpp:113] Setting up loss
I0928 21:59:57.496443 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.496464 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.496479 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.496484 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.496487 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.496500 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.496503 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.496506 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.496521 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.496531 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.496547 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.496549 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.496564 31416 solver.cpp:42] Solver scaffolding done.
I0928 21:59:57.496618 31416 solver.cpp:32] Initializing solver from parameters: 
test_iter: 10
test_interval: 10
base_lr: 0.01
display: 100
max_iter: 100
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
net: "/tmp/tmpsemI8J"
snapshot_after_train: false
I0928 21:59:57.496628 31416 solver.cpp:70] Creating training net from net file: /tmp/tmpsemI8J
I0928 21:59:57.496743 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.496785 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.496805 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.496822 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.496829 31416 net.cpp:342] data -> data
I0928 21:59:57.496846 31416 net.cpp:342] data -> label
I0928 21:59:57.496860 31416 net.cpp:113] Setting up data
I0928 21:59:57.496878 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.496884 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.496887 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.496892 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.496906 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.496912 31416 net.cpp:384] conv <- data
I0928 21:59:57.496924 31416 net.cpp:342] conv -> conv
I0928 21:59:57.496937 31416 net.cpp:113] Setting up conv
I0928 21:59:57.496968 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.496973 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.496987 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.496999 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.497004 31416 net.cpp:384] ip <- conv
I0928 21:59:57.497016 31416 net.cpp:342] ip -> ip
I0928 21:59:57.497027 31416 net.cpp:113] Setting up ip
I0928 21:59:57.497732 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.497737 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.497750 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.497758 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.497763 31416 net.cpp:384] loss <- ip
I0928 21:59:57.497771 31416 net.cpp:384] loss <- label
I0928 21:59:57.497779 31416 net.cpp:342] loss -> loss
I0928 21:59:57.497789 31416 net.cpp:113] Setting up loss
I0928 21:59:57.497795 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.497815 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.497820 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.497824 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.497828 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.497833 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.497835 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.497838 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.497843 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.497851 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.497858 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.497860 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.497958 31416 solver.cpp:154] Creating test net (#0) specified by net file: /tmp/tmpsemI8J
I0928 21:59:57.498011 31416 net.cpp:42] Initializing net from parameters: 
name: "testnet"
force_backward: true
state {
  phase: TEST
}
layer {
  name: "data"
  type: "DummyData"
  top: "data"
  top: "label"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 1
    }
    data_filler {
      type: "constant"
    }
    num: 5
    num: 5
    channels: 2
    channels: 1
    height: 3
    height: 1
    width: 4
    width: 1
  }
}
layer {
  name: "conv"
  type: "Convolution"
  bottom: "data"
  top: "conv"
  param {
    decay_mult: 1
  }
  param {
    decay_mult: 0
  }
  convolution_param {
    num_output: 11
    pad: 3
    kernel_size: 2
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
      value: 2
    }
  }
}
layer {
  name: "ip"
  type: "InnerProduct"
  bottom: "conv"
  top: "ip"
  inner_product_param {
    num_output: 13
    weight_filler {
      type: "gaussian"
      std: 2.5
    }
    bias_filler {
      type: "constant"
      value: -3
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip"
  bottom: "label"
  top: "loss"
}
I0928 21:59:57.498052 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.498070 31416 layer_factory.hpp:74] Creating layer data
I0928 21:59:57.498083 31416 net.cpp:84] Creating Layer data
I0928 21:59:57.498090 31416 net.cpp:342] data -> data
I0928 21:59:57.498107 31416 net.cpp:342] data -> label
I0928 21:59:57.498123 31416 net.cpp:113] Setting up data
I0928 21:59:57.498144 31416 net.cpp:120] Top shape: 5 2 3 4 (120)
I0928 21:59:57.498150 31416 net.cpp:120] Top shape: 5 1 1 1 (5)
I0928 21:59:57.498153 31416 net.cpp:126] Memory required for data: 500
I0928 21:59:57.498158 31416 layer_factory.hpp:74] Creating layer conv
I0928 21:59:57.498172 31416 net.cpp:84] Creating Layer conv
I0928 21:59:57.498177 31416 net.cpp:384] conv <- data
I0928 21:59:57.498189 31416 net.cpp:342] conv -> conv
I0928 21:59:57.498203 31416 net.cpp:113] Setting up conv
I0928 21:59:57.498236 31416 net.cpp:120] Top shape: 5 11 8 9 (3960)
I0928 21:59:57.498241 31416 net.cpp:126] Memory required for data: 16340
I0928 21:59:57.498255 31416 layer_factory.hpp:74] Creating layer ip
I0928 21:59:57.498268 31416 net.cpp:84] Creating Layer ip
I0928 21:59:57.498273 31416 net.cpp:384] ip <- conv
I0928 21:59:57.498284 31416 net.cpp:342] ip -> ip
I0928 21:59:57.498294 31416 net.cpp:113] Setting up ip
I0928 21:59:57.499011 31416 net.cpp:120] Top shape: 5 13 (65)
I0928 21:59:57.499017 31416 net.cpp:126] Memory required for data: 16600
I0928 21:59:57.499028 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.499037 31416 net.cpp:84] Creating Layer loss
I0928 21:59:57.499042 31416 net.cpp:384] loss <- ip
I0928 21:59:57.499050 31416 net.cpp:384] loss <- label
I0928 21:59:57.499058 31416 net.cpp:342] loss -> loss
I0928 21:59:57.499068 31416 net.cpp:113] Setting up loss
I0928 21:59:57.499074 31416 layer_factory.hpp:74] Creating layer loss
I0928 21:59:57.499099 31416 net.cpp:120] Top shape: (1)
I0928 21:59:57.499104 31416 net.cpp:122]     with loss weight 1
I0928 21:59:57.499107 31416 net.cpp:126] Memory required for data: 16604
I0928 21:59:57.499112 31416 net.cpp:167] loss needs backward computation.
I0928 21:59:57.499116 31416 net.cpp:167] ip needs backward computation.
I0928 21:59:57.499119 31416 net.cpp:167] conv needs backward computation.
I0928 21:59:57.499122 31416 net.cpp:169] data does not need backward computation.
I0928 21:59:57.499126 31416 net.cpp:208] This network produces output loss
I0928 21:59:57.499135 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.499141 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.499145 31416 net.cpp:222] Memory required for data: 16604
I0928 21:59:57.499158 31416 solver.cpp:42] Solver scaffolding done.
I0928 21:59:57.499488 31416 solver.cpp:247] Solving testnet
I0928 21:59:57.499495 31416 solver.cpp:248] Learning Rate Policy: inv
I0928 21:59:57.499506 31416 solver.cpp:291] Iteration 0, Testing net (#0)
I0928 21:59:57.499511 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.499514 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.499519 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.499523 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.500375 31416 solver.cpp:340]     Test net output #0: loss = 80.246 (* 1 = 80.246 loss)
I0928 21:59:57.500672 31416 solver.cpp:214] Iteration 0, loss = 83.9572
I0928 21:59:57.500684 31416 solver.cpp:229]     Train net output #0: loss = 83.9572 (* 1 = 83.9572 loss)
I0928 21:59:57.500690 31416 solver.cpp:492] Iteration 0, lr = 0.01
I0928 21:59:57.504210 31416 solver.cpp:291] Iteration 10, Testing net (#0)
I0928 21:59:57.504218 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.504221 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.504225 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.504228 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.505053 31416 solver.cpp:340]     Test net output #0: loss = 80.1163 (* 1 = 80.1163 loss)
I0928 21:59:57.507647 31416 solver.cpp:291] Iteration 20, Testing net (#0)
I0928 21:59:57.507657 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.507660 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.507664 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.507668 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.508487 31416 solver.cpp:340]     Test net output #0: loss = 80.2366 (* 1 = 80.2366 loss)
I0928 21:59:57.511106 31416 solver.cpp:291] Iteration 30, Testing net (#0)
I0928 21:59:57.511114 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.511117 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.511121 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.511124 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.511945 31416 solver.cpp:340]     Test net output #0: loss = 87.0918 (* 1 = 87.0918 loss)
I0928 21:59:57.515373 31416 solver.cpp:291] Iteration 40, Testing net (#0)
I0928 21:59:57.515398 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.515403 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.515406 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.515409 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.516310 31416 solver.cpp:340]     Test net output #0: loss = 78.2672 (* 1 = 78.2672 loss)
I0928 21:59:57.518975 31416 solver.cpp:291] Iteration 50, Testing net (#0)
I0928 21:59:57.518985 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.518988 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.518992 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.518996 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.519829 31416 solver.cpp:340]     Test net output #0: loss = 77.4133 (* 1 = 77.4133 loss)
I0928 21:59:57.525305 31416 solver.cpp:291] Iteration 60, Testing net (#0)
I0928 21:59:57.525313 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.525316 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.525321 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.525323 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.526166 31416 solver.cpp:340]     Test net output #0: loss = 85.1151 (* 1 = 85.1151 loss)
I0928 21:59:57.530853 31416 solver.cpp:291] Iteration 70, Testing net (#0)
I0928 21:59:57.530899 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.530906 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.530917 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.530922 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.532475 31416 solver.cpp:340]     Test net output #0: loss = 83.8431 (* 1 = 83.8431 loss)
I0928 21:59:57.537236 31416 solver.cpp:291] Iteration 80, Testing net (#0)
I0928 21:59:57.537271 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.537277 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.537287 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.537293 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.538851 31416 solver.cpp:340]     Test net output #0: loss = 79.2561 (* 1 = 79.2561 loss)
I0928 21:59:57.544770 31416 solver.cpp:291] Iteration 90, Testing net (#0)
I0928 21:59:57.544790 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.544796 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.544805 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.544811 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.546355 31416 solver.cpp:340]     Test net output #0: loss = 79.4704 (* 1 = 79.4704 loss)
I0928 21:59:57.550884 31416 solver.cpp:273] Iteration 100, loss = 50.1075
I0928 21:59:57.550909 31416 solver.cpp:291] Iteration 100, Testing net (#0)
I0928 21:59:57.550916 31416 net.cpp:639] Copying source layer data
I0928 21:59:57.550921 31416 net.cpp:639] Copying source layer conv
I0928 21:59:57.550930 31416 net.cpp:639] Copying source layer ip
I0928 21:59:57.550935 31416 net.cpp:639] Copying source layer loss
I0928 21:59:57.552399 31416 solver.cpp:340]     Test net output #0: loss = 79.558 (* 1 = 79.558 loss)
I0928 21:59:57.552429 31416 solver.cpp:278] Optimization Done.
.I0928 21:59:57.552922 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "data"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 10
  dim: 9
  dim: 8
}
layer {
  name: "one"
  type: "Python"
  bottom: "data"
  top: "one"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
layer {
  name: "two"
  type: "Python"
  bottom: "one"
  top: "two"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
layer {
  name: "three"
  type: "Python"
  bottom: "two"
  top: "three"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
I0928 21:59:57.553001 31416 net.cpp:344] Input 0 -> data
I0928 21:59:57.553048 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.553081 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.553143 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.553154 31416 net.cpp:384] one <- data
I0928 21:59:57.553174 31416 net.cpp:342] one -> one
I0928 21:59:57.553192 31416 net.cpp:113] Setting up one
I0928 21:59:57.553268 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.553277 31416 net.cpp:126] Memory required for data: 2880
I0928 21:59:57.553287 31416 layer_factory.hpp:74] Creating layer two
I0928 21:59:57.553323 31416 net.cpp:84] Creating Layer two
I0928 21:59:57.553333 31416 net.cpp:384] two <- one
I0928 21:59:57.553350 31416 net.cpp:342] two -> two
I0928 21:59:57.553369 31416 net.cpp:113] Setting up two
I0928 21:59:57.553416 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.553424 31416 net.cpp:126] Memory required for data: 5760
I0928 21:59:57.553431 31416 layer_factory.hpp:74] Creating layer three
I0928 21:59:57.553467 31416 net.cpp:84] Creating Layer three
I0928 21:59:57.553477 31416 net.cpp:384] three <- two
I0928 21:59:57.553494 31416 net.cpp:342] three -> three
I0928 21:59:57.553509 31416 net.cpp:113] Setting up three
I0928 21:59:57.553555 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.553563 31416 net.cpp:126] Memory required for data: 8640
I0928 21:59:57.553571 31416 net.cpp:169] three does not need backward computation.
I0928 21:59:57.553576 31416 net.cpp:169] two does not need backward computation.
I0928 21:59:57.553580 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.553586 31416 net.cpp:208] This network produces output three
I0928 21:59:57.553601 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.553606 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.553611 31416 net.cpp:222] Memory required for data: 8640
.I0928 21:59:57.556713 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "data"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 10
  dim: 9
  dim: 8
}
layer {
  name: "one"
  type: "Python"
  bottom: "data"
  top: "one"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
layer {
  name: "two"
  type: "Python"
  bottom: "one"
  top: "two"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
layer {
  name: "three"
  type: "Python"
  bottom: "two"
  top: "three"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
I0928 21:59:57.556799 31416 net.cpp:344] Input 0 -> data
I0928 21:59:57.556852 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.556886 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.556953 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.556965 31416 net.cpp:384] one <- data
I0928 21:59:57.556985 31416 net.cpp:342] one -> one
I0928 21:59:57.557004 31416 net.cpp:113] Setting up one
I0928 21:59:57.557080 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.557088 31416 net.cpp:126] Memory required for data: 2880
I0928 21:59:57.557097 31416 layer_factory.hpp:74] Creating layer two
I0928 21:59:57.557137 31416 net.cpp:84] Creating Layer two
I0928 21:59:57.557147 31416 net.cpp:384] two <- one
I0928 21:59:57.557164 31416 net.cpp:342] two -> two
I0928 21:59:57.557183 31416 net.cpp:113] Setting up two
I0928 21:59:57.557232 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.557240 31416 net.cpp:126] Memory required for data: 5760
I0928 21:59:57.557247 31416 layer_factory.hpp:74] Creating layer three
I0928 21:59:57.557283 31416 net.cpp:84] Creating Layer three
I0928 21:59:57.557293 31416 net.cpp:384] three <- two
I0928 21:59:57.557312 31416 net.cpp:342] three -> three
I0928 21:59:57.557327 31416 net.cpp:113] Setting up three
I0928 21:59:57.557382 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.557390 31416 net.cpp:126] Memory required for data: 8640
I0928 21:59:57.557397 31416 net.cpp:169] three does not need backward computation.
I0928 21:59:57.557402 31416 net.cpp:169] two does not need backward computation.
I0928 21:59:57.557407 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.557413 31416 net.cpp:208] This network produces output three
I0928 21:59:57.557427 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.557433 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.557437 31416 net.cpp:222] Memory required for data: 8640
.I0928 21:59:57.560763 31416 net.cpp:42] Initializing net from parameters: 
name: "pythonnet"
input: "data"
force_backward: true
state {
  phase: TRAIN
}
input_shape {
  dim: 10
  dim: 9
  dim: 8
}
layer {
  name: "one"
  type: "Python"
  bottom: "data"
  top: "one"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
layer {
  name: "two"
  type: "Python"
  bottom: "one"
  top: "two"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
layer {
  name: "three"
  type: "Python"
  bottom: "two"
  top: "three"
  python_param {
    module: "test_python_layer"
    layer: "SimpleLayer"
  }
}
I0928 21:59:57.560852 31416 net.cpp:344] Input 0 -> data
I0928 21:59:57.560900 31416 net.cpp:67] Memory required for data: 0
I0928 21:59:57.560935 31416 layer_factory.hpp:74] Creating layer one
I0928 21:59:57.560994 31416 net.cpp:84] Creating Layer one
I0928 21:59:57.561005 31416 net.cpp:384] one <- data
I0928 21:59:57.561024 31416 net.cpp:342] one -> one
I0928 21:59:57.561041 31416 net.cpp:113] Setting up one
I0928 21:59:57.561110 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.561117 31416 net.cpp:126] Memory required for data: 2880
I0928 21:59:57.561126 31416 layer_factory.hpp:74] Creating layer two
I0928 21:59:57.561161 31416 net.cpp:84] Creating Layer two
I0928 21:59:57.561170 31416 net.cpp:384] two <- one
I0928 21:59:57.561188 31416 net.cpp:342] two -> two
I0928 21:59:57.561205 31416 net.cpp:113] Setting up two
I0928 21:59:57.561266 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.561275 31416 net.cpp:126] Memory required for data: 5760
I0928 21:59:57.561293 31416 layer_factory.hpp:74] Creating layer three
I0928 21:59:57.561327 31416 net.cpp:84] Creating Layer three
I0928 21:59:57.561336 31416 net.cpp:384] three <- two
I0928 21:59:57.561352 31416 net.cpp:342] three -> three
I0928 21:59:57.561367 31416 net.cpp:113] Setting up three
I0928 21:59:57.561409 31416 net.cpp:120] Top shape: 10 9 8 (720)
I0928 21:59:57.561416 31416 net.cpp:126] Memory required for data: 8640
I0928 21:59:57.561437 31416 net.cpp:169] three does not need backward computation.
I0928 21:59:57.561442 31416 net.cpp:169] two does not need backward computation.
I0928 21:59:57.561446 31416 net.cpp:169] one does not need backward computation.
I0928 21:59:57.561453 31416 net.cpp:208] This network produces output three
I0928 21:59:57.561466 31416 net.cpp:449] Collecting Learning Rate and Weight Decay.
I0928 21:59:57.561472 31416 net.cpp:221] Network initialization done.
I0928 21:59:57.561476 31416 net.cpp:222] Memory required for data: 8640
.
----------------------------------------------------------------------
Ran 15 tests in 0.272s

OK
